---
jupytext:
  hide_notebook_metadata: true
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: vasca-github
  language: python
  name: vasca-github
---

```{code-cell}
:tags: [remove-cell]

# ruff: noqa: T201
```

```{code-cell}
:tags: [remove-input]

import pandas as pd
from IPython.display import HTML, display
from itables import init_notebook_mode, show

init_notebook_mode(all_interactive=True)

# Modify Table CSS so with colors that work ok in light and dark themes
class_specific_css = """
.dataTable th {
    font-weight: normal;
    background-color: #075;
    color: #fff;
}
.dataTable td {
        border-color: #f0f;
        background-color: #333;
        color: #fff;
}
.dt-container {
  font-size: small;
}

"""
display(HTML(f"<style>{class_specific_css}</style>"))
```

# Pipeline

This is a tutorial showcasing VASCA's pipeline flow on a simple example. We will go
through all the steps equivalent to what is done in [](#vasca_pipe.run_from_file).
This is the same function that is called when starting the pipeline from the CLI using ``vasca-pipe``.

The goal is to create a VASCA [](#Region) from multiple [](#GALEXField) for which we
download the raw data online from [MAST](https://astroquery.readthedocs.io/en/latest/mast/mast.html).
We apply quality cuts and do source clustering followed by variability analysis and
finally source cross-matching.

For this tutorial we are interested in the near-ultraviolet (NUV) observations
by GALEX. We are going to look at neighboring/overlapping fields all of which
contain the location of famous Tidal Disruption Event [_PS1-10jh_](https://en.wikipedia.org/wiki/Pan-STARRS#Selected_discoveries) discovered by Pan-STARRS and observed by GALEX in 2010.

:::{figure-md} galex-fields-ps1-10jh
<img src="../images/GALEX_fields_ps1-10jh.jpg" alt="galex_fields_ps1-10jh" class="bg-primary mb-1" width="400px">

GALEX sky map with field footprints of observations around the location of PS1-10jh (purple
crosshair). Sreenshot from [MAST Portal](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html)
:::

+++

## General Configuration

The standard pipeline processing starts by reading a yaml file. To keep this tutorial
simple, we are going to introduce parts of the configuration step by step at the point
where they are required in the pipeline flow.

```{note}
An important premise of the configuration is that each parameter needs to be
configured explicitly. This means even default values are specified all the time. This
is a design decision purposefully made in order to ensure transparent and complete
configurations. As a result, all possible parameters are always included when looking
at configuration file.
```

Let's begin with the ``general`` section. Here, basic information and functionality is
configured. The ``name`` of the pipeline run specifies also the name of directory in
which all results will be stored. The location of output directory is at ``out_dir_base``
relative to the root directory of the package.

VASCA uses the powerful logging system provided by [loguru](https://loguru.readthedocs.io/en/stable/index.html).
The configuration specifies the [``log_level``](https://loguru.readthedocs.io/en/stable/api/logger.html#loguru._logger.Logger.level),
which we are going to set to debugging mode here. By default VASCA is going to save
all logging messages in a file stored in the output directory. ``log_file`` specifies
the name of that file, while ``default`` tells the pipeline to use a default name.

Parallel processing of the field-level analysis can be enabled when setting the number
of CPU threads ``nr_cpus > 1``.

VASCA can include field-averaged reference data, if such data is available additional
to the visit-level data from the instruments mission pipeline. To save memory/storage
and computation time it is configurable wether to include reference sources in the
final [](#Region)-file (``save_ref_srcs``) and to repeat already processed fields that
are included in the region (``run_fields``).

```{code-cell}
# Dictionary holding the configuration
config = {}

# General section of the configuration
config["general"] = {
    "name": "simple_pipe",
    "out_dir_base": "docs/tutorial_resources/vasca_pipeline",
    "log_level": "DEBUG",
    "log_file": "default",
    "nr_cpus": 3,
    "save_ref_srcs": True,
    "run_fields": True,
}
```

:::{tip}
In case the output location is on a remote server and multiple users should be
able to edit the data, i.e., reprocess data using an updated configruation, then
one needs to manage user access priviliges. For convenience this can be done
using [``umask``](https://en.wikipedia.org/wiki/Umask):
```Python
import os
os.umask("0o003", 0)
```
This will grand user and group full permissions. The setting is only persistant
throughout the Python session.
:::

+++

The pipeline begins with some prerequisites including enabling logging and creating
the output directory

```{code-cell}
import sys
from loguru import logger
from pathlib import Path
from importlib.resources import files

# Setup output directory
out_dir_base = Path(files("vasca").parent / config["general"]["out_dir_base"])
out_dir_base.mkdir(parents=True, exist_ok=True)

pipe_dir = out_dir_base / config["general"]["name"]
pipe_dir.mkdir(parents=True, exist_ok=True)

# Path to log file
log_file = (
    pipe_dir / f'{config["general"]["name"]}.log'
)  # Pipeline name is used by default

# Logger configuration, both to stdout and .log file
log_cfg = {
    "handlers": [
        {
            "sink": sys.stdout,
            "format": "<green>{time:YYYY-MM-DD HH:mm:ss.SSSS}</green> "
            "<cyan>{name}</cyan>:<cyan>{line}</cyan> |"
            "<level>{level}:</level> {message}",
            "level": config["general"]["log_level"],
            "colorize": True,
            "backtrace": True,
            "diagnose": True,
        },
    ],
}
logger.configure(**log_cfg)  # Set config
logger.add(log_file)  # Add file logger
logger.enable("vasca")  # Enable logging

# Some initial logging messages
logger.info(f"Runing '{config['general']['name']}'")
logger.debug(f"Config. file: {log_file}")
```

## Resources
Next is the section about resource handling. This specifies the method used to load
(``load_method``) field data and which data products should be included (tables,
tables plus visit-level images, or only metadata ``load_products``). Additionally
the ``coadd_exists`` flag tells the pipeline wether it can expect co-added (field-
averaged) data. Finally, ``field_kwargs`` allows to pass parameters directly to
the ``init`` function of a field class.

Here we are going to initialize fields from local raw data, if present. Else the
required data is downloaded from MAST. All data products will be included including
co-add data. Using the [](#ResourceManager) we can tell the field class where to
locate the data.

```{code-cell}
from vasca.resource_manager import ResourceManager

# Get the data locations using ResourceManager
rm = ResourceManager()
data_path = rm.get_path("docs_resources", "vasca")
visits_data_path = rm.get_path("gal_visits_list", "vasca")

# Resource section of the configuration
config["resources"] = {
    "load_method": "MAST_LOCAL",
    "load_products": "ALL",
    "coadd_exists": True,
    "field_kwargs": {
        "data_path": data_path,
        "visits_data_path": visits_data_path,
    },
}
```

## Observations
The observations section of the configuration is responsible for configuring
which combination of instrument (``observatory``) and filter (``obs_filter``)to
load data. Also it specifies the exact list of fields to load (``obs_field_ids``).

In a later step we will also add here the selection parameters (``selection``) used
for the quality cuts on the data and the field-level clustering settings
(``cluster_det``).

```{code-cell}
config["observations"] = [
    {
        "observatory": "GALEX",
        "obs_filter": "NUV",
        "obs_field_ids": [
            3880803393752530944,  # MISGCSN2_10493_0117
            2529969795944677376,  # ELAISN1_09
            2597664528044916736,  # PS_ELAISN1_MOS15
        ],
        # "cluster_det": {},
        # "selection": {},
    },
    # More instruments/filters...
]
```

Find below the visit metadata about the fields under investigation.

```{code-cell}
:tags: [hide-input]

from astropy.table import Table

df_gal_visits = (
    Table.read(visits_data_path)
    .to_pandas()
    .apply(lambda x: x.str.decode("utf-8") if x.dtype == "O" else x)
)
query = f"ParentImgRunID in {list(config['observations'][0]['obs_field_ids'])}"
df_select = df_gal_visits.query(query)
show(
    df_select,
    classes="display nowrap compact",
    scrollY="300px",
    scrollCollapse=True,
    paging=False,
    columnDefs=[{"className": "dt-body-left", "targets": "_all"}],
)
```

In the next step we will initialize a VASCA [](#Region) with all fields sequentially.
[](#load_from_config) is a convenience function that acts as an interface between the
region object and field-specific loading functions. This will downloads the data from
MAST, it will detect if the data is already present on disc and loads the cashed
files. To safe compute time later, a VASCA-field file is written to the download
location so that one can use this file instead of creating a new field from raw data.
This will be used during the field-level [processing](#field-level-analysis).

```{code-cell}
:tags: [hide-output]

from vasca.region import Region

rg = Region.load_from_config(config)
```

This populates the region object with all specified fields, the relevant metadata is
stored in {term}`tt_fields`.

```{code-cell}
rg.info()
# rg.tt_fields
```

```{code-cell}
:tags: [remove_input]

df_tt_fields = rg.tt_fields.to_pandas().apply(
    lambda x: x.str.decode("utf-8") if x.dtype == "O" else x
)
show(
    df_tt_fields,
    classes="display nowrap compact",
    scrollY="300px",
    scrollCollapse=True,
    paging=False,
    columnDefs=[{"className": "dt-body-left", "targets": "_all"}],
)
```

## Field-level analysis

The field-level analysis incorporates, first, the data reduction and parameter mapping
from raw data to VASCA field objects, second, the data quality selection and finally
source clustering on the remaining high-quality detections.

The first step is implicitly taken care of by the [](#GALEXField) class, where the raw
data is loaded and only the column parameters are kept that are specified in the [](#tables_dict)
module. A shortcut is provided through the [](#Region.get_field) method which is an
interface to the ``load`` method of
any field class.

The configuration for the next two step requires the ``selection`` and ``cluster_det``
entries under the observations section.

### Data selection
```{note}
A crucial part of VASCA's flexibility to adapt to raw data of virtually any instrument
comes from the fact that the parameter list used for data quality selection is not
fixed and is allowed to vary for different instruments and filters. The only
requirement is an existent entry in the [](#tables_dict) module for any parameter and
a corresponding field class that includes these parameters in the {term}`tt_detections`
table.
```

The API for the data selection is provided by the [](#TableCollection.select_rows)
method. Each entry under selection maps to this interface. The ``table`` parameters
specifies which table to select on. Any selection operation modifies the ``sel``
column of a given table. It contains boolean values so ``0`` means _unselected_ and
``1`` means _selected_.

By specifying the ``presel_type``parameter, one controls the logic by which an
existing selection is combined with a new one. The ``sel_type`` parameter specifies
the logic by which the selection on a set of multiple column parameters is combined.
Parameters ``range`` and ``bitmask`` provide the column parameter and artifact
bitflag values that are used to make the selection. Using ``set_range`` on can choose
to clip values of a certain column to minimum and maximum values.

In combination with ``sel_type = "is_in"`` and ``var`` parameters, it is possible to
select the rows of given column ``var``in the target table if a value is also present
in the same column of a reference table (``ref_table``).

```{code-cell}
import numpy as np

# Updating the observations for GALEX-NUV observations
config["observations"][0].update(
    {
        "selection": {
            # Quality cuts on visit-level detections
            "det_quality": {
                "table": "tt_detections",
                "presel_type": "and",
                "sel_type": "and",
                "range": {
                    "s2n": [3.0, np.inf],
                    "r_fov": [0.0, 0.5],
                    "ellip_world": [0.0, 0.5],
                    "size_world": [0.0, 6.0],
                    "class_star": [0.15, 1.0],
                    "chkobj_type": [-0.5, 0.5],
                    "flux_app_ratio": [0.3, 1.05],
                },
                "bitmask": {
                    "artifacts": [2, 4, 8, 128, 256],
                },
                "set_range": {"pos_err": [0.5, 5]},
            },
            # Quality cuts on field-averaged detections
            "coadd_det_quality": {
                "table": "tt_detections",
                "presel_type": "and",
                "sel_type": "and",
                "range": {
                    "s2n": [5.0, np.inf],
                    "r_fov": [0.0, 0.5],
                    "ellip_world": [0.0, 0.5],
                    "size_world": [0.0, 6.0],
                    "class_star": [0.15, 1.0],
                    "chkobj_type": [-0.5, 0.5],
                    "flux_app_ratio": [0.3, 1.05],
                },
                "bitmask": {
                    "artifacts": [2, 4, 8, 128, 256],
                },
            },
            # Selection on only those detections wich are part of clusters
            "det_association": {
                "table": "tt_detections",
                "presel_type": "and",
                "sel_type": "is_in",
                "ref_table": "tt_sources",
                "var": "fd_src_id",
            },
        },
    }
)
```

### Clustering

Also the field-level clustering configuration showcases VASCA's modular
approach. In the ``cluster_det`` section on specifies the clustering algorithm
which, in principle, can be different for each instrument and filter. Although,
at the moment only [mean-shift clustering](https://en.wikipedia.org/wiki/Mean_shift)
is supported by VASCA.

Again, the responsible API is provided by [](#TableCollection.cluster_meanshift).
This method wraps a method provided by the [scikit-learn package](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html). The end result is that each field optains
a new {term}`tt_visits` table that lists all identified sources as defined
by their culstered detections. Sources have at the minimum one and as manny as ``n_vis``
detections.

Mean-shift is well suited for this use case due to several resons. Most importantly
it is that the algorithm doesn't require the total number of clusters as a parameter.
In fact it is determining that number which would be otherwise very difficult to
predict from the visit-level detections before having done the clustering.

Another reason is its relatively simple algrorithm where only one parameters is
required. It is called the ``bandwidth`` which means, translated to the astronomy use
case, the radial size of a typical source on the sky. It should be roughly chosen
to match the instrument's PSF, which, for GALEX, is about 5 arcseconds. We set
it slightly smaller to limit false associations also considering that the
soure center is usually much better constrained than the PSF might suggest.

```{code-cell}
# Updating the observations for GALEX-NUV observations continued...
config["observations"][0].update(
    {
        "cluster_det": {
            "meanshift": {
                "bandwidth": 4,
                "seeds": None,
                "bin_seeding": False,
                "min_bin_freq": 1,
                "cluster_all": True,
                "n_jobs": None,
                "max_iter": 300,
                "table_name": "tt_detections",
            },
        },
    },
)
```

### Pipeline flow
According to the configuration above, we can finally run the analysis. VASCA
implements parallel processing ([](inv:#*.Pool.starmap)) for this part of the pipeline
by applying the [](#run_field) method in parallel for each field.

```{code-cell}
import vasca.utils as vutils

# Collect parameters from config
fd_pars: list = []
vobs: list[dict] = config["observations"]

obs_nr: int
field_nr: str
# Loop over observation list index
for obs_nr, _ in enumerate(vobs):
    # Loop over fields
    for field_nr in vobs[obs_nr]["obs_field_ids"]:
        # Construct VASCA field ID (prepending instrument/filter identifier)
        iprefix: str = vutils.dd_obs_id_add[
            vobs[obs_nr]["observatory"] + vobs[obs_nr]["obs_filter"]
        ]
        field_id: str = f"{iprefix}{field_nr}"
        # Save parameters outside loop
        fd_pars.append([obs_nr, field_id, rg, config])
```

```{code-cell}
from multiprocessing.pool import Pool
import vasca.vasca_pipe as vpipe

# Run each field in a separate process in parallel
nr_cpus = config["general"]["nr_cpus"]
logger.info(f"Analyzing {len(fd_pars)} fields on {nr_cpus} parallel threads.")

with Pool(processes=nr_cpus) as pool:
    pool_return = pool.starmap(vpipe.run_field_docs, fd_pars)
pool.join()

logger.info("Done analyzing individual fields.")
```

```{code-cell}
# To be continued ...
```
